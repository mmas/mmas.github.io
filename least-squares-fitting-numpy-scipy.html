<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/Article">
  <head>
    <title>Least squares fitting with Numpy and Scipy</title>
    

    <meta charset="utf-8">
    <meta property="og:title" content="Least squares fitting with Numpy and Scipy">
    <meta property="og:site_name" content="Modesto Mas | Blog">
    <meta property="og:image" content="https://mmas.github.io/images/profile.jpg">
    <meta property="og:image:width" content="200">
    <meta property="og:image:height" content="200">
    <meta property="og:url" content="https://mmas.github.io/least-squares-fitting-numpy-scipy">
    <meta property="og:locale" content="en_GB">
    <meta name="twitter:image" content="https://mmas.github.io/images/profile.jpg">
    <meta name="twitter:url" content="https://mmas.github.io/least-squares-fitting-numpy-scipy">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:domain" content="mmas.github.io">
    <meta name="twitter:title" content="Least squares fitting with Numpy and Scipy">
    <meta name="description" content="Both Numpy and Scipy provide black box methods to fit one-dimensional data using linear least squares, in the first case, and non-linear least squares...">
    <meta name="twitter:description" content="Both Numpy and Scipy provide black box methods to fit one-dimensional data using linear least squares, in the first case, and non-linear least squares...">
    <meta property="og:description" content="Both Numpy and Scipy provide black box methods to fit one-dimensional data using linear least squares, in the first case, and non-linear least squares...">
    <meta name="keywords" content="numerical-analysis,numpy,optimization,python,scipy">
    <meta property="og:type" content="blog">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
<meta property="og:type" content="article">
<meta property="article:author" content="https://github.com/mmas">
<meta property="article:section" content="numerical-analysis">
<meta property="article:tag" content="numerical-analysis,numpy,optimization,python,scipy">
<meta property="article:published_time" content="2015-11-11">
<meta property="article:modified_time" content="2015-11-11">

    <link rel="stylesheet" type="text/css" href="/css/main.css">
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        CommonHTML: {
            scale: 93,
            showMathMenu: false
        },
        tex2jax: {
            "inlineMath": [["$","$"], ["\\(","\\)"]]
        }
    });
    </script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  </head>
  <body class="entry-detail">

    
    <header>
  <div>
    <img src="https://mmas.github.io/images/profile.jpg">
    <a class="brand" href="/">Modesto Mas</a>
    <span>Numerical Computing, Python, Julia, Hadoop and more</span>
    <nav>
      <ul>
      <li><a href="/tags">Tags</a></li>
      </ul>
    </nav>
  </div>
</header>
    

    <section id="content" role="main">
    

<article>
  
<header>
  <h1><a href="/least-squares-fitting-numpy-scipy">Least squares fitting with Numpy and Scipy</a></h1>
  <time datetime="2015-11-11">Nov 11, 2015</time>
  
  <a class="tag" href="/tags?tag=numerical-analysis">numerical-analysis</a>
  
  <a class="tag" href="/tags?tag=numpy">numpy</a>
  
  <a class="tag" href="/tags?tag=optimization">optimization</a>
  
  <a class="tag" href="/tags?tag=python">python</a>
  
  <a class="tag" href="/tags?tag=scipy">scipy</a>
  
</header>

  <section class="body">
    <p>Both <a target="_blank" href="http://www.numpy.org/">Numpy</a> and <a target="_blank" href="http://scipy.org/scipylib/index.html">Scipy</a> provide black box methods to fit one-dimensional data using <a target="_blank" href="https://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29">linear least squares</a>, in the first case, and <a target="_blank" href="https://en.wikipedia.org/wiki/Non-linear_least_squares">non-linear least squares</a>, in the latter. Let's dive into them:</p>
<pre><code class="python">import numpy as np
from scipy import optimize
import matplotlib.pyplot as plt
</code></pre>

<h2>Linear least squares fitting</h2>
<p>Our <a target="_blank" href="https://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29">linear least squares</a> fitting problem can be defined as a system of <em>m</em> linear equations and <em>n</em> coefficents with <em>m &gt; n</em>. In a vector notation, this will be:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>X</mi>
  <mo>=</mo>
  <mrow>
    <mo>[</mo>
    <mtable rowspacing="4pt" columnspacing="1em">
      <mtr>
        <mtd>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>11</mn>
            </mrow>
          </msub>
        </mtd>
        <mtd>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>12</mn>
            </mrow>
          </msub>
        </mtd>
        <mtd>
          <mo>&#x22EF;<!-- ⋯ --></mo>
        </mtd>
        <mtd>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>1</mn>
              <mi>n</mi>
            </mrow>
          </msub>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>21</mn>
            </mrow>
          </msub>
        </mtd>
        <mtd>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>22</mn>
            </mrow>
          </msub>
        </mtd>
        <mtd>
          <mo>&#x22EF;<!-- ⋯ --></mo>
        </mtd>
        <mtd>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>2</mn>
              <mi>n</mi>
            </mrow>
          </msub>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mo>&#x22EE;<!-- ⋮ --></mo>
        </mtd>
        <mtd>
          <mo>&#x22EE;<!-- ⋮ --></mo>
        </mtd>
        <mtd>
          <mo>&#x22F1;<!-- ⋱ --></mo>
        </mtd>
        <mtd>
          <mo>&#x22EE;<!-- ⋮ --></mo>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>m</mi>
              <mn>1</mn>
            </mrow>
          </msub>
        </mtd>
        <mtd>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>m</mi>
              <mn>2</mn>
            </mrow>
          </msub>
        </mtd>
        <mtd>
          <mo>&#x22EF;<!-- ⋯ --></mo>
        </mtd>
        <mtd>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>m</mi>
              <mi>n</mi>
            </mrow>
          </msub>
        </mtd>
      </mtr>
    </mtable>
    <mo>]</mo>
  </mrow>
  <mo>,</mo>
  <mtext>&#xA0;</mtext>
  <mi>&#x03B2;<!-- β --></mi>
  <mo>=</mo>
  <mrow>
    <mo>[</mo>
    <mtable rowspacing="4pt" columnspacing="1em">
      <mtr>
        <mtd>
          <msub>
            <mi>&#x03B2;<!-- β --></mi>
            <mn>1</mn>
          </msub>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <msub>
            <mi>&#x03B2;<!-- β --></mi>
            <mn>2</mn>
          </msub>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mo>&#x22EE;<!-- ⋮ --></mo>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <msub>
            <mi>&#x03B2;<!-- β --></mi>
            <mi>n</mi>
          </msub>
        </mtd>
      </mtr>
    </mtable>
    <mo>]</mo>
  </mrow>
  <mo>,</mo>
  <mtext>&#xA0;</mtext>
  <mi>y</mi>
  <mo>=</mo>
  <mrow>
    <mo>[</mo>
    <mtable rowspacing="4pt" columnspacing="1em">
      <mtr>
        <mtd>
          <msub>
            <mi>y</mi>
            <mn>1</mn>
          </msub>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <msub>
            <mi>y</mi>
            <mn>2</mn>
          </msub>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mo>&#x22EE;<!-- ⋮ --></mo>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <msub>
            <mi>y</mi>
            <mi>m</mi>
          </msub>
        </mtd>
      </mtr>
    </mtable>
    <mo>]</mo>
  </mrow>
</math>
<p>The <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>X</mi>
</math> matrix corresponds to a <a target="_blank" href="https://en.wikipedia.org/wiki/Vandermonde_matrix">Vandermonde  matrix</a> of our <code>x</code> variable, but in our case, instead of the first column, we will set our last one to ones in the variable <code>a</code>. Doing this and for consistency with the next examples, the result will be the array <code>[m, c]</code> instead of <code>[c, m]</code> for the linear equation</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>y</mi>
  <mo>=</mo>
  <mi>m</mi>
  <mi>x</mi>
  <mo>+</mo>
  <mi>c</mi>
  <mo>.</mo>
</math>
<p>To get our best estimated coefficients we will need to solve the minimization problem</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow class="MJX-TeXAtom-ORD">
    <mover>
      <mi>&#x03B2;<!-- β --></mi>
      <mo stretchy="false">&#x005E;<!-- ^ --></mo>
    </mover>
  </mrow>
  <mo>=</mo>
  <munder>
    <mrow class="MJX-TeXAtom-ORD">
      <mi mathvariant="normal">a</mi>
      <mi mathvariant="normal">r</mi>
      <mi mathvariant="normal">g</mi>
      <mi mathvariant="normal">m</mi>
      <mi mathvariant="normal">i</mi>
      <mi mathvariant="normal">n</mi>
    </mrow>
    <mi>&#x03B2;<!-- β --></mi>
  </munder>
  <mtext>&#xA0;</mtext>
  <mo>&#x2225;<!-- ∥ --></mo>
  <mi>y</mi>
  <mo>&#x2212;<!-- − --></mo>
  <mi>X</mi>
  <mi>&#x03B2;<!-- β --></mi>
  <msup>
    <mo fence="false" stretchy="false">&#x2225;<!-- ∥ --></mo>
    <mn>2</mn>
  </msup>
</math>
<p>by solving the equation</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow class="MJX-TeXAtom-ORD">
    <mover>
      <mi>&#x03B2;<!-- β --></mi>
      <mo stretchy="false">&#x005E;<!-- ^ --></mo>
    </mover>
  </mrow>
  <mo>=</mo>
  <mo stretchy="false">(</mo>
  <msup>
    <mi>X</mi>
    <mi>T</mi>
  </msup>
  <mi>X</mi>
  <msup>
    <mo stretchy="false">)</mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mo>&#x2212;<!-- − --></mo>
      <mn>1</mn>
    </mrow>
  </msup>
  <msup>
    <mi>X</mi>
    <mi>T</mi>
  </msup>
  <mi>y</mi>
</math>
<p>We can do this directly with Numpy. Let's create an example of noisy data first:</p>
<pre><code class="python">f = np.poly1d([5, 1])

x = np.linspace(0, 10, 30)
y = f(x) + 6*np.random.normal(size=len(x))
xn = np.linspace(0, 10, 200)

plt.plot(x, y, 'or')
plt.show()
</code></pre>

<p><img alt="Linear function data" src="/images/linear_function_data.png"></p>
<p>To solve the equation with Numpy:</p>
<pre><code class="python">a = np.vstack([x, np.ones(len(x))]).T
np.dot(np.linalg.inv(np.dot(a.T, a)), np.dot(a.T, y))
</code></pre>

<pre><code class="stdout">array([ 5.59418256, -1.37189559])
</code></pre>

<p>We can use the <a target="_blank" href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html"><code>lstsqs</code></a> function from the <a target="_blank" href="http://docs.scipy.org/doc/numpy/reference/routines.linalg.html"><code>linalg</code></a> module to do the same:</p>
<pre><code class="python">np.linalg.lstsq(a, y)[0]
</code></pre>

<pre><code class="stdout">array([ 5.59418256, -1.37189559])
</code></pre>

<p>And, easier, with the <code>polynomial</code> module:</p>
<pre><code class="python">np.polyfit(x, y, 1)
</code></pre>

<pre><code class="stdout">array([ 5.59418256, -1.37189559])
</code></pre>

<p>As we can see, all of them calculate a good aproximation to the coefficients of the original function.</p>
<pre><code class="python">m, c = np.polyfit(x, y, 1)
yn = np.polyval([m, c], xn)

plt.plot(x, y, 'or')
plt.plot(xn, yn)
plt.show()
</code></pre>

<p><img alt="Linear function data fit" src="/images/linear_function_data_fit.png"></p>
<p>In terms of speed, the first method is the fastest and the last one, a bit slower than the second method:</p>
<pre><code class="python">def leastsq1(x):
    a = np.vstack([x, np.ones(len(x))]).T
    return np.dot(np.linalg.inv(np.dot(a.T, a)), np.dot(a.T, y))

def leastsq2(x):
    a = np.vstack([x, np.ones(len(x))]).T
    return np.linalg.lstsq(np.vstack([x, np.ones(len(x))]).T, y)[0]

def leastsq3(x):
    return np.polyfit(x, y, 1)
</code></pre>

<pre><code class="python">%timeit leastsq1(x)
</code></pre>

<pre><code class="stdout">The slowest run took 8.36 times longer than the fastest. This could mean that an intermediate result is being cached
100000 loops, best of 3: 16 µs per loop
</code></pre>

<pre><code class="python">%timeit leastsq2(x)
</code></pre>

<pre><code class="stdout">The slowest run took 5.15 times longer than the fastest. This could mean that an intermediate result is being cached
10000 loops, best of 3: 58.8 µs per loop
</code></pre>

<pre><code class="python">%timeit leastsq3(x)
</code></pre>

<pre><code class="stdout">The slowest run took 4.43 times longer than the fastest. This could mean that an intermediate result is being cached
10000 loops, best of 3: 73.1 µs per loop
</code></pre>

<h2>Polynomial fitting</h2>
<p>In the case of polynomial functions the fitting can be done in the same way as the linear functions. Using <a target="_blank" href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html"><code>polyfit</code></a>, like in the previous example, the array <code>x</code> will be converted in a Vandermonde matrix of the size <code>(n, m)</code>, being <code>n</code> the number of coefficients (the degree of the polymomial plus one) and <code>m</code> the lenght of the data array.</p>
<p>Just to introduce the example and for using it in the next section, let's fit a polynomial function:</p>
<pre><code class="python">f = np.poly1d([-5, 1, 3])
x = np.linspace(0, 2, 20)
y = f(x) + 1.5*np.random.normal(size=len(x))
xn = np.linspace(0, 2, 200)

plt.plot(x, y, 'or')
plt.show()
</code></pre>

<p><img alt="Polynomial function data" src="/images/polynomial_function_data.png"></p>
<pre><code class="python">popt = np.polyfit(x, y, 2)
print popt
</code></pre>

<pre><code class="stdout">[-4.23466637 -1.0709698   4.51393962]
</code></pre>

<pre><code class="python">yn = np.polyval(popt, xn)

plt.plot(x, y, 'or')
plt.plot(xn, yn)
plt.show()
</code></pre>

<p><img alt="Polynomial function data linear fit" src="/images/polynomial_function_data_fit.png"></p>
<p>The speed results:</p>
<pre><code class="python">%timeit np.polyfit(x, y, 2)
</code></pre>

<pre><code class="stdout">The slowest run took 5.03 times longer than the fastest. This could mean that an intermediate result is being cached
10000 loops, best of 3: 79.4 µs per loop
</code></pre>

<h2>Non-linear fitting</h2>
<p>In this section we are going back to <a target="_blank" href="/optimization_scipy">the previous post</a> and make use of the <code>optimize</code> module of Scipy to fit data with non-linear equations.</p>
<p>Scipy's least square function uses <a target="_blank" href="https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm">Levenberg-Marquardt algorithm</a> to solve a non-linear leasts square problems. Levenberg-Marquardt algorithm is an iterative method to find local minimums. We'll need to provide a initial guess (<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x03B2;<!-- β --></mi>
</math>) and, in each step, the guess will be estimated as <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x03B2;<!-- β --></mi>
  <mo>+</mo>
  <mi>&#x03B4;<!-- δ --></mi>
</math> determined by</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>f</mi>
  <mo stretchy="false">(</mo>
  <msub>
    <mi>x</mi>
    <mi>i</mi>
  </msub>
  <mo>,</mo>
  <mi>&#x03B2;<!-- β --></mi>
  <mo>+</mo>
  <mi>&#x03B4;<!-- δ --></mi>
  <mo stretchy="false">)</mo>
  <mo>&#x2248;<!-- ≈ --></mo>
  <mi>f</mi>
  <mo stretchy="false">(</mo>
  <msub>
    <mi>x</mi>
    <mi>i</mi>
  </msub>
  <mo>,</mo>
  <mi>&#x03B2;<!-- β --></mi>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <msub>
    <mi>J</mi>
    <mi>i</mi>
  </msub>
  <mi>&#x03B4;<!-- δ --></mi>
  <mo>,</mo>
</math>
<p>being <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msub>
    <mi>J</mi>
    <mi>i</mi>
  </msub>
</math> the gradient of the cost function with respect <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x03B2;<!-- β --></mi>
</math>.</p>
<p>This gradient will be zero at the minimum of the sum squares and then, the coefficients (<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x03B2;<!-- β --></mi>
</math>) will be the best estimated. In vector notation:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <munder>
    <mrow class="MJX-TeXAtom-ORD">
      <mi mathvariant="normal">a</mi>
      <mi mathvariant="normal">r</mi>
      <mi mathvariant="normal">g</mi>
      <mi mathvariant="normal">m</mi>
      <mi mathvariant="normal">i</mi>
      <mi mathvariant="normal">n</mi>
    </mrow>
    <mrow>
      <mi>&#x03B2;<!-- β --></mi>
      <mo>+</mo>
      <mi>&#x03B4;<!-- δ --></mi>
    </mrow>
  </munder>
  <mtext>&#xA0;</mtext>
  <mo>&#x2225;<!-- ∥ --></mo>
  <mi>y</mi>
  <mo>&#x2212;<!-- − --></mo>
  <mi>f</mi>
  <mo stretchy="false">(</mo>
  <mi>&#x03B2;<!-- β --></mi>
  <mo stretchy="false">)</mo>
  <mo>&#x2212;<!-- − --></mo>
  <mi>J</mi>
  <mi>&#x03B4;<!-- δ --></mi>
  <msup>
    <mo>&#x2225;<!-- ∥ --></mo>
    <mn>2</mn>
  </msup>
  <mo>=</mo>
  <mn>0.</mn>
</math>
<p>This will be solved as:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mo stretchy="false">(</mo>
  <msup>
    <mi>J</mi>
    <mi>T</mi>
  </msup>
  <mi>J</mi>
  <mo>+</mo>
  <mi>&#x03BB;<!-- λ --></mi>
  <mi>diag</mi>
  <mo>&#x2061;<!-- ⁡ --></mo>
  <mo stretchy="false">(</mo>
  <msup>
    <mi>J</mi>
    <mi>T</mi>
  </msup>
  <mi>J</mi>
  <mo stretchy="false">)</mo>
  <mo stretchy="false">)</mo>
  <mi>&#x03B4;<!-- δ --></mi>
  <mo>=</mo>
  <msup>
    <mi>J</mi>
    <mi>T</mi>
  </msup>
  <mo stretchy="false">[</mo>
  <mi>y</mi>
  <mo>&#x2212;<!-- − --></mo>
  <mi>f</mi>
  <mo stretchy="false">(</mo>
  <mi>&#x03B2;<!-- β --></mi>
  <mo stretchy="false">)</mo>
  <mo stretchy="false">]</mo>
  <mo>,</mo>
</math>
<p>being <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x03BB;<!-- λ --></mi>
</math> the dumping factor (<code>factor</code> argument in the Scipy implementation).</p>
<p>Here is the implementation of the previous example. A function definition is used instead of the previous polynomial definition for a better performance and the residual function corresponds to the function to minimize the error, <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>y</mi>
  <mo>&#x2212;<!-- − --></mo>
  <mi>f</mi>
  <mo stretchy="false">(</mo>
  <mi>&#x03B2;<!-- β --></mi>
  <mo stretchy="false">)</mo>
</math> in the previous equation:</p>
<pre><code class="python">def f(x, a, b, c):
    return a*x**2 + b*x + c

def residual(p, x, y):
    return y - f(x, *p)

p0 = [1., 1., 1.]
popt, pcov = optimize.leastsq(residual, p0, args=(x, y))

print popt
</code></pre>

<pre><code class="stdout">[-5.98229569  3.14299536  2.16551107]
</code></pre>

<pre><code class="python">yn = f(xn, *popt)

plt.plot(x, y, 'or')
plt.plot(xn, yn)
plt.show()
</code></pre>

<p><img alt="Polynomial function data non-linear fit" src="/images/polynomial_function_data_fit_nonlinear.png"></p>
<p>In terms of speed, we'll have similar results to the linear least squares in this case:</p>
<pre><code class="python">%timeit optimize.leastsq(residual, p0, args=(x, y))
</code></pre>

<pre><code class="stdout">10000 loops, best of 3: 79.3 µs per loop
</code></pre>

<p>In the following examples, non-polynomial functions will be used and the solution of the problems must be done using non-linear solvers. Also, we will compare the non-linear least square fitting with the optimizations seen in the previous post.</p>
<h3>Exponential functions</h3>
<p>Here is the data we are going to work with:</p>
<pre><code class="python">def f(x, b, c):
    return b**x+c

p = [1.6, 10]
x = np.linspace(0, 6, 20)
y = f(x, *p) + np.random.normal(size=len(x))
xn = np.linspace(0, 6, 200)

plt.plot(x, y, 'or')
plt.show()
</code></pre>

<p><img alt="Exponential function data" src="/images/exponential_function_data.png"></p>
<p>The non-linear least squares fit:</p>
<pre><code class="python">def residual(p, x, y):
    return y - f(x, *p)

p0 = [1., 8.]
popt, pcov = optimize.leastsq(residual, p0, args=(x, y))

print popt

yn = f(xn, *popt)

plt.plot(x, y, 'or')
plt.plot(xn, yn)
plt.show()
</code></pre>

<pre><code class="stdout">[  1.60598173  10.05263527]
</code></pre>

<p><img alt="Exponential function data fit" src="/images/exponential_function_data_fit.png"></p>
<p>We should use non-linear least squares if the dimensionality of the output vector is larger than the number of parameters to optimize. Here, we can see the number of function evaluations of our last estimation of the coeffients:</p>
<pre><code class="python">popt, pcov, info, mesg, ler = optimize.leastsq(residual, p0, args=(x, y), full_output=True)
print info['nfev']
</code></pre>

<pre><code class="stdout">23
</code></pre>

<p>Using as a example, a L-BFGS minimization we will achieve the minimization in more cost function evaluations:</p>
<pre><code class="python">def min_residual(p, x, y):
    return sum(residual(p, x, y)**2)

res = optimize.minimize(min_residual, p0, method='L-BFGS-B', args=(x, y))
print res.x
print res.nfev
</code></pre>

<pre><code class="stdout">[  1.60598173  10.05263545]
60
</code></pre>

<p>An easier interface for non-linear least squares fitting is using Scipy's <a target="_blank" href="docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"><code>curve_fit</code></a>. <code>curve_fit</code> uses <code>leastsq</code> with the default residual function (the same we defined previously) and an initial guess of <code>[1.]*n</code>, being <code>n</code> the number of coefficients required (number of objective function arguments minus one):</p>
<pre><code class="python">popt, pcov = optimize.curve_fit(f, x, y)
print popt
</code></pre>

<pre><code class="stdout">[  1.60598173  10.05263527]
</code></pre>

<p>In the speed comparison we can see a better performance for the <code>leastqs</code> function:</p>
<pre><code class="python">%timeit optimize.leastsq(residual, p0, args=(x, y))
</code></pre>

<pre><code class="stdout">1000 loops, best of 3: 166 µs per loop
</code></pre>

<pre><code class="python">%timeit optimize.curve_fit(f, x, y, p0=p0)
</code></pre>

<pre><code class="stdout">1000 loops, best of 3: 277 µs per loop
</code></pre>

<h3>Trigonometric functions</h3>
<p>Let's define some noised data from a trigonometric function:</p>
<pre><code class="python">def f(x, a, b):
    return a*np.sin(b*np.pi*x)

p = [5, 5]
x = np.linspace(0, 1, 30)
y = f(x, *p) + .5*np.random.normal(size=len(x))
xn = np.linspace(0, 1, 200)

plt.plot(x, y, 'or')
plt.show()
</code></pre>

<p><img alt="Trigonometric function data" src="/images/trigonometric_function_data.png"></p>
<p>Fitting the data with non-linear least squares:</p>
<pre><code class="python">popt, pcov = optimize.curve_fit(f, x, y)

print popt

plt.plot(x, y, 'or')
plt.plot(xn, f(xn, *popt))
plt.show()
</code></pre>

<p><img alt="Trigonometric function data bad fit" src="/images/trigonometric_function_data_bad_fit.png"></p>
<p>We obtained a really bad fitting, in this case we will need a better initial guess. Observing the data we have it is possible to set a better initial estimation:</p>
<pre><code class="python">p0 = [3, 4]
popt, pcov = optimize.curve_fit(f, x, y, p0=p0)
print popt

plt.plot(x, y, 'or')
plt.plot(xn, f(xn, *popt))
plt.show()
</code></pre>

<p><img alt="Trigonometric function data fit" src="/images/trigonometric_function_data_fit.png"></p>
<p>And the speed comparison for this function we observe similar results than the previous example:</p>
<pre><code class="python">%timeit optimize.leastsq(residual, p0, args=(x, y))
</code></pre>

<pre><code class="stdout">10000 loops, best of 3: 163 µs per loop
</code></pre>

<pre><code class="python">%timeit optimize.curve_fit(f, x, y, p0=p0)
</code></pre>

<pre><code class="stdout">1000 loops, best of 3: 281 µs per loop
</code></pre>
  </section>
</article>


    </section>
    <footer></footer>

    
  </body>
</html>