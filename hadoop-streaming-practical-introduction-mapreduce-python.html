<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/Article">
  <head>
    <title>Hadoop Streaming. Practical introduction to MapReduce with Python</title>
    

    <meta charset="utf-8">
    <meta property="og:title" content="Hadoop Streaming. Practical introduction to MapReduce with Python">
    <meta property="og:site_name" content="Modesto Mas | Blog">
    <meta property="og:image" content="https://mmas.github.io/images/profile.jpg">
    <meta property="og:image:width" content="200">
    <meta property="og:image:height" content="200">
    <meta property="og:url" content="https://mmas.github.io/hadoop-streaming-practical-introduction-mapreduce-python">
    <meta property="og:locale" content="en_GB">
    <meta name="twitter:image" content="https://mmas.github.io/images/profile.jpg">
    <meta name="twitter:url" content="https://mmas.github.io/hadoop-streaming-practical-introduction-mapreduce-python">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:domain" content="mmas.github.io">
    <meta name="twitter:title" content="Hadoop Streaming. Practical introduction to MapReduce with Python">
    <meta name="description" content="Apache Hadoop is a framework for distributed storage and processing. I&#39;m not going to explain how Hadoop modules work or to describe the Hadoop ecosys...">
    <meta name="twitter:description" content="Apache Hadoop is a framework for distributed storage and processing. I&#39;m not going to explain how Hadoop modules work or to describe the Hadoop ecosys...">
    <meta property="og:description" content="Apache Hadoop is a framework for distributed storage and processing. I&#39;m not going to explain how Hadoop modules work or to describe the Hadoop ecosys...">
    <meta name="keywords" content="data-processing,hadoop,mapreduce,python">
    <meta property="og:type" content="blog">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
<meta property="og:type" content="article">
<meta property="article:author" content="https://github.com/mmas">
<meta property="article:section" content="data-processing">
<meta property="article:tag" content="data-processing,hadoop,mapreduce,python">
<meta property="article:published_time" content="2015-09-11">
<meta property="article:modified_time" content="2015-09-11">

    <link rel="stylesheet" type="text/css" href="/css/main.css">
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        CommonHTML: {
            scale: 93,
            showMathMenu: false
        },
        tex2jax: {
            "inlineMath": [["$","$"], ["\\(","\\)"]]
        }
    });
    </script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  </head>
  <body class="entry-detail">

    
    <header>
  <div>
    <img src="https://mmas.github.io/images/profile.jpg">
    <a class="brand" href="/">Modesto Mas</a>
    <span>Numerical Computing, Python, Julia, Hadoop and more</span>
    <nav>
      <ul>
      <li><a href="/tags">Tags</a></li>
      </ul>
    </nav>
  </div>
</header>
    

    <section id="content" role="main">
    

<article>
  
<header>
  <h1><a href="/hadoop-streaming-practical-introduction-mapreduce-python">Hadoop Streaming. Practical introduction to MapReduce with Python</a></h1>
  <time datetime="2015-09-11">Sep 11, 2015</time>
  
  <a class="tag" href="/tags?tag=data-processing">data-processing</a>
  
  <a class="tag" href="/tags?tag=hadoop">hadoop</a>
  
  <a class="tag" href="/tags?tag=mapreduce">mapreduce</a>
  
  <a class="tag" href="/tags?tag=python">python</a>
  
</header>

  <section class="body">
    <p><a target="_blank" href="http://hadoop.apache.org/">Apache Hadoop</a> is a framework for distributed storage and processing. I'm not going to explain how Hadoop modules work or to describe the Hadoop ecosystem, since there are a lot of really good resources that you can easily find in the form of blog entries, papers, books or videos.</p>
<p>Hadoop is written in Java, however, for these two MapReduce examples I'm going to use Python for the mapper and reducer functions. You can use any language that can read and write standard input and outputs for the Hadoop Streaming.</p>
<h2>Maximum temperature</h2>
<p>Obtain the maximum temperature of each day of 1998.</p>
<p>I'm going to use some weather data from <a target="_blank" href="http://www.ncdc.noaa.gov/">NCDC</a>. Without any reason I've chosen the daily records of 1998. A month is represented per file and I'm going to focus on the date (second column) and maximum temperature (third column) to get the maximum temperature according to the different weather stations (wban - first column).</p>
<p>You can download the data at <a target="_blank" href="http://www.ncdc.noaa.gov/orders/qclcd/">http://www.ncdc.noaa.gov/orders/qclcd/</a>. </p>
<p>Create sample data to test:</p>
<pre>% head -n 100 199801daily.txt &gt;&gt; sample.txt
</pre>

<p>Set executable permissions:</p>
<pre>% chmod +x mapper.py reducer.py
</pre>

<p>Mapper function:</p>
<pre>#!/usr/bin/env python

import sys


for line in sys.stdin:
    if not line.strip() or line.startswith('Wban'):
        continue
    _, k, v = line[:17].split(',', 2)
    try:
        v = int(v)
    except ValueError:
        continue
    print '%s\t%s' % (k, v)
</pre>

<p>Reducer function:</p>
<pre>#!/usr/bin/env python

import sys


last_k = None
max_v = 0

for line in sys.stdin:
    k, v = line.strip().split('\t')
    v = int(v)

    if last_k == k:
        max_v = max(max_v, v)
    else:
        if last_k:
            print '%s\t%s' % (last_k, max_v)
        last_k = k
        max_v = v

if last_k == k:
    print '%s\t%s' % (last_k, max_v)
</pre>

<p>Test:</p>
<pre>% cat sample.txt | ./mapper.py  | sort | ./reducer.py
</pre>

<p>Copy data from local filesystem to HDFS:</p>
<pre>% hadoop fs -put 1998*.txt /user/$(whoami)/input/
</pre>

<p>Map and reduce with Hadoop:</p>
<pre>% hadoop jar $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -input input/1998*.txt \
  -output output \
  -mapper /opt/hadoop/maximum_temperature/mapper.py \
  -reducer /opt/hadoop/maximum_temperature/reducer.py
</pre>

<p>See the results:</p>
<pre>% hadoop fs -cat output/*

[...]
19981222    94
19981223    92
19981224    91
19981225    91
19981226    90
19981227    91
19981228    90
19981229    90
19981230    89
19981231    89
</pre>

<h2>Word counter</h2>
<p>Count the words of a book.</p>
<p>From a book in text file I'm going to count the times a word is repeated. This is a simple example, so some issues won't be covered (like word contractions).</p>
<p>The book I've chosen is <em>The Trial </em>and you can download it from <a target="_blank" href="http://www.gutenberg.org/ebooks/7849">http://www.gutenberg.org/ebooks/7849</a></p>
<p>Create sample data:</p>
<pre>% head -n 100 the_trial__franz_kafka.txt &gt;&gt; sample.txt
</pre>

<p>Set executable permissions:</p>
<pre>% chmod +x mapper.py reducer.py
</pre>

<p>Mapper function:</p>
<pre>#!/usr/bin/env python

import re
import sys


started = False

for line in sys.stdin:
    if started:
        if line.startswith('*** END OF THIS PROJECT'):
            break
        # Filter out some punctuation marks and set to lowercase.
        line = re.sub(r'[&quot;?!.,;:()-]', '', line).strip().lower()
        for word in line.split():
            print '%s\t1' % word
    elif line.startswith('*** START OF THIS PROJECT'):
        started = True
</pre>

<p>Reducer function:</p>
<pre>#!/usr/bin/env python

import sys


last_k = None
last_v = 0

for line in sys.stdin:
    k, v = line.strip().split('\t')
    v = int(v)

    if last_k == k:
        last_v += v
    else:
        if last_k:
            print '%s\t%s' % (last_k, last_v)
        last_k = k
        last_v = v

if last_k == k:
    print '%s\t%s' % (last_k, last_v)
</pre>

<p>Test:</p>
<pre>% cat sample.txt | ./mapper.py  | sort | ./reducer.py
</pre>

<p>Copy data from local filesystem to HDFS:</p>
<pre>% hadoop fs -put the_trial__franz_kafka.txt /user/$(whoami)/input/
</pre>

<p>Map and reduce with Hadoop:</p>
<pre>% hadoop jar $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -input input/the_trial__franz_kafka.txt \
  -output output \
  -mapper /opt/hadoop/word_count/mapper.py \
  -reducer /opt/hadoop/word_count/reducer.py
</pre>

<p>See the results:</p>
<pre>% hadoop fs -cat output/*

[...]
you'll    12
you're    67
you've    37
young    30
younger    1
your        94
yours    10
yourself        23
yourselves    3
youth    4
</pre>
  </section>
</article>


    </section>
    <footer></footer>

    
  </body>
</html>